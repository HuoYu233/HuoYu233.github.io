<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>《动手深度学习》阅读笔记 | HawYiorのBlog</title>
  <link rel="stylesheet" href="/css/fonts/Chinese-normal-normal.min.css">
  <link rel="stylesheet" href="/css/fonts/ChineseMono-normal-normal.min.css">
  <link rel="stylesheet" href="/css/fonts/Chinese-italic-normal.min.css">
  <link rel="stylesheet" href="/css/fonts/Chinese-normal-bold.min.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="description" content="《动手深度学习》书籍笔记">
  
  
  
    <link rel="shortcut icon" href="/logo.png">
  
  <link rel="stylesheet" href="/css/style.css">
  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div id="nav-outer">
  <nav id="main-nav" class="outer">
    <a id="main-nav-toggle" class="nav-icon"></a>
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
      <a class="main-nav-link" href="/log">Log</a>
    
    <div class="main-nav-space-between"></div>
    
  </nav>
</div>
<div id="header-title">
  <h1 id="logo-wrap">
    <a href="/" id="logo">HawYiorのBlog</a>
  </h1>
  
    <h2 id="subtitle-wrap">
      <a href="/" id="subtitle">we are here to code the world!</a>
    </h2>
  
</div>

      <div id="content" class="outer">
        <section id="main"><article id="post-dive-into-deep-learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/dive-into-deep-learning/" class="article-date">
  <time class="dt-published" datetime="2024-01-22T13:52:00.000Z" itemprop="datePublished">2024-01-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      《动手深度学习》阅读笔记
    </h1>
  

      </header>
    
    
<div id="article-toc">
    <h2 class="widget-title">目录</h2>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text">1. 预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 数据处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 线性代数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">1.3.</span> <span class="toc-text">1.2 自动微分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-%E6%A6%82%E7%8E%87"><span class="toc-number">1.4.</span> <span class="toc-text">1.3 概率</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.</span> <span class="toc-text">2. 线性神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#21-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-softmax"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 softmax</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">3.</span> <span class="toc-text">3. 多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#31-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-%E8%AF%AF%E5%B7%AE"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 误差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#33-%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 权重衰退</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#34-%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="toc-number">3.4.</span> <span class="toc-text">3.4 丢弃法</span></a></li></ol></li></ol>
</div>

    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-预备知识">1. 预备知识</h1>
<h2 id="11-数据处理">1.1 数据处理</h2>
<p><code>Tensor</code>数据类型和numpy中的<code>ndarray</code>类型相似<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是差异点在于</p>
<p>首先<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>GPU很好地支持加速计算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而NumPy仅支持CPU计算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span></p>
<p>其次<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>张量类支持自动微分<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这些功能使得张量类更适合深度学习</p>
<h2 id="12-线性代数">1.2 线性代数</h2>
<ul>
<li><em>Hadamard</em>积</li>
</ul>
<p>两个矩阵的按元素乘法称为<em>Hadamard积</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Hadamard product<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>数学符号⊙<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
<p><span class="markdown-them-math-inline">$A⊙B = \begin{bmatrix}
 a_{11}b_{11} &amp; a_{12}b_{12} &amp;a_{13}b_{13} \\
 a_{21}b_{21} &amp; a_{22}b_{22} &amp;a_{23}b_{23} \\
  a_{31}b_{31}&amp;a_{32}b_{32}  &amp;a_{3,3}b_{33}
\end{bmatrix}$</span></p>
<p>将张量乘以或加上一个标量不会改变张量的形状<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其中张量的每个元素都将与标量相加或相乘<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<ul>
<li>非降维求和</li>
</ul>
<p>如果我们想沿某个轴计算<code>A</code>元素的累积总和<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 比如<code>axis=0</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>按行计算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以调用<code>cumsum</code>函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 此函数不会沿任何轴降低输入张量的维度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">A<span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<ul>
<li>点积</li>
</ul>
<p>给定两个向量<span class="markdown-them-math-inline">$x,y$</span>的点积<span class="markdown-them-math-inline">$x^Ty$</span>(或<span class="markdown-them-math-inline">$&lt;x,y&gt;$</span>)是相同位置的按元素乘积的和</p>
<p><span class="markdown-them-math-inline">$x^Ty = \sum_{i=1}^{d}x_iy_i$</span></p>
<p>将两个向量规范化得到单位长度后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>点积表示它们夹角的余弦<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<ul>
<li>矩阵-向量积</li>
</ul>
<p><span class="markdown-them-math-inline">$Ax = \begin{bmatrix}
 a_1^T \\
 a_2^T \\
a_3^T
\end{bmatrix}x = \begin{bmatrix}
 a_1^Tx \\
 a_2^Tx \\
a_3^Tx
\end{bmatrix}$</span></p>
<p>在代码中使用张量表示矩阵-向量积<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们使用<code>mv</code>函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<ul>
<li>矩阵-矩阵乘法</li>
</ul>
<p>用行向量<span class="markdown-them-math-inline">$A_i^T$</span>表示矩阵<span class="markdown-them-math-inline">$A$</span>的第<span class="markdown-them-math-inline">$i$</span>行<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>列向量<span class="markdown-them-math-inline">$b_j$</span>作为矩阵<span class="markdown-them-math-inline">$B$</span>的第<span class="markdown-them-math-inline">$j$</span>列</p>
<p>看作简单地执行m次矩阵-向量积<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并将结果拼接在一起<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用<code>mm</code>函数</p>
<p><span class="markdown-them-math-inline">$C=AB= \begin{bmatrix}
 a_1^T \\
 a_2^T \\
a_3^T
\end{bmatrix}\begin{bmatrix}
 b_1 &amp; b_2 &amp; b_3\\
\end{bmatrix} = \begin{bmatrix}
 a_1^Tb_1 &amp; a_1^Tb_2 &amp; a_1^Tb_3\\
 a_2^Tb_1 &amp; a_2^Tb_2 &amp; a_2^Tb_3 \\
a_3^Tb_1 &amp; a_3^Tb_2 &amp; a_3^Tb_3
\end{bmatrix}$</span></p>
<ul>
<li>范数</li>
</ul>
<p>欧几里得距离是一个<span class="markdown-them-math-inline">$L_2$</span>范数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span> 假设<span class="markdown-them-math-inline">$n$</span>维向量<span class="markdown-them-math-inline">$x$</span>中的元素是<span class="markdown-them-math-inline">$x_1,x_2...x_n$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其<span class="markdown-them-math-inline">$L_2$</span><em>范数</em>是向量元素平方和的平方根<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><span class="markdown-them-math-inline">$||x||_2 = \sqrt{\sum_{i=1}^{n}x_i^2}$</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">u <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>u<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">5.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>深度学习中更经常地使用<span class="markdown-them-math-inline">$L_2$</span>范数的平方<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也会经常遇到<span class="markdown-them-math-inline">$L_1$</span>范数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它表示为向量元素的绝对值之和<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><span class="markdown-them-math-inline">$||x||_1 = \sum_{i=1}^{n}|x_i|$</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>u<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>矩阵<span class="markdown-them-math-inline">$X$</span>的<em>Frobenius范数</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Frobenius norm<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>是矩阵元素平方和的平方根</p>
<h2 id="12-自动微分">1.2 自动微分</h2>
<ul>
<li>计算图</li>
<li>反向传播</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">4.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># None</span>
y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad <span class="token operator">==</span> <span class="token number">4</span> <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token comment">#tensor([True, True, True, True])</span>
<span class="token comment"># 在默认情况下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>PyTorch会累积梯度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们需要清除之前的值</span>
x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>当<code>y</code>不是标量时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>向量<code>y</code>关于向量<code>x</code>的导数的最自然解释是一个矩阵<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 对于高阶和高维的<code>y</code>和<code>x</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>求导的结果可以是一个高阶张量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>然而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>虽然这些更奇特的对象确实出现在高级机器学习中<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>包括深度学习中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 但当调用向量的反向计算时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这里<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们的目的不是计算微分矩阵<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而是单独计算批量中每个样本的偏导数之和<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 对非标量调用backward需要传入一个gradient参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>该参数指定微分函数关于self的梯度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></span>
<span class="token comment"># 本例只想求偏导数的和<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以传递一个1的梯度是合适的</span>
x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> x <span class="token operator">*</span> x
<span class="token comment"># 等价于y.backward(torch.ones(len(x)))</span>
y<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
x<span class="token punctuation">.</span>grad <span class="token comment"># tensor([0., 2., 4., 6.])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>有时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们希望将某些计算移动到记录的计算图之外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>假设<code>y</code>是作为<code>x</code>的函数计算的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而<code>z</code>则是作为<code>y</code>和<code>x</code>的函数计算的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 想象一下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们想计算<code>z</code>关于<code>x</code>的梯度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但由于某种原因<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>希望将<code>y</code>视为一个常数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 并且只考虑到<code>x</code>在<code>y</code>被计算后发挥的作用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>这里可以分离<code>y</code>来返回一个新变量<code>u</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>该变量与<code>y</code>具有相同的值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 但丢弃计算图中如何计算<code>y</code>的任何信息<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 换句话说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>梯度不会向后流经<code>u</code>到<code>x</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 因此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>下面的反向传播函数计算<code>z=u*x</code>关于<code>x</code>的偏导数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>同时将<code>u</code>作为常数处理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 而不是<code>z=x*x*x</code>关于<code>x</code>的偏导数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> x <span class="token operator">*</span> x
u <span class="token operator">=</span> y<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
z <span class="token operator">=</span> u <span class="token operator">*</span> x

z<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
x<span class="token punctuation">.</span>grad <span class="token operator">==</span> u <span class="token comment"># tensor([True, True, True, True])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="13-概率">1.3 概率</h2>
<p>[TODO]</p>
<h1 id="2-线性神经网络">2. 线性神经网络</h1>
<h2 id="21-线性回归">2.1 线性回归</h2>
<p>定义<span class="markdown-them-math-inline">$\vec{x} = \begin{bmatrix}
  x_1 &amp; x_2 &amp; x_3 &amp; ...
\end{bmatrix}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>参数<span class="markdown-them-math-inline">$\vec{w} = \begin{bmatrix}
  w_1 &amp; w_2 &amp; w_3 &amp; ...
\end{bmatrix}$</span></p>
<p><span class="markdown-them-math-inline">$\hat{y} = Xw+b$</span></p>
<p>损失函数<span class="markdown-them-math-inline">$J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} {(y^i-\hat{y})}^2$</span></p>
<p>随机梯度下降<code>SGD(stochastic gradient descent)</code></p>
<h2 id="22-softmax">2.2 softmax</h2>
<ul>
<li>
<p><em>独热编码</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>one-hot encoding<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
<p>独热编码是一个向量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它的分量和类别一样多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 类别对应的分量设置为1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其他所有分量设置为0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
</ul>
<p><img src="https://zh.d2l.ai/_images/softmaxreg.svg" alt="../_images/softmaxreg.svg"></p>
<p>要将输出视为概率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们必须保证在任何数据上的输出都是非负的且总和为1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 此外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们需要一个训练的目标函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>来激励模型精准地估计概率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 在分类器输出0.5的所有样本中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们希望这些样本是刚好有一半实际上属于预测的类别<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这个属性叫做<em>校准</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>calibration<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>softmax函数能够将未规范化的预测变换为非负数并且总和为1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>同时让模型保持 可导的性质<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 为了完成这一目标<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们首先对每个未规范化的预测求幂<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这样可以确保输出非负<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 为了确保最终输出的概率值总和为1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们再让每个求幂后的结果除以它们的总和<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如下式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><span class="markdown-them-math-inline">$\hat{y} = softmax(o)$</span></p>
<p>其中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\hat{y}_j = \frac{exp(o_j)}{\sum_{k}{exp(o_k)}}$</span></p>
<p>在预测过程中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们仍然可以用下式来选择最有可能的类别<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><span class="markdown-them-math-inline">$argmax\ \hat{y}_j = argmax\ o_j$</span></p>
<p>尽管softmax是一个非线性函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但softmax回归的输出仍然由输入特征的仿射变换决定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 因此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>softmax回归是一个<em>线性模型</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>linear model<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>其中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对于任何标签<span class="markdown-them-math-inline">$y$</span>和模型预测<span class="markdown-them-math-inline">$\hat{y}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>损失函数为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><span class="markdown-them-math-inline">$l(y,\hat{y})=-\sum_{j=q}^{q}y_jlog\hat{y}_j$</span></p>
<p>通常被称为<em>交叉熵损失</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>cross-entropy loss<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
<h1 id="3-多层感知机">3. 多层感知机</h1>
<h2 id="31-激活函数">3.1 激活函数</h2>
<p>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 使其能处理更普遍的函数关系类型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 要做到这一点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最简单的方法是将许多全连接层堆叠在一起<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 每一层都输出到上面的层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>直到生成最后的输出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 我们可以把前<span class="markdown-them-math-inline">$L-1$</span>层看作表示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>把最后一层看作线性预测器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这种架构通常称为<em>多层感知机</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>multilayer perceptron<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常缩写为<em>MLP</em><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并通过激活函数转换隐藏层的输出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="https://zh.d2l.ai/_images/mlp.svg" alt="../_images/mlp.svg"></p>
<p><em>激活函数</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>activation function<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>通过计算加权和并加上偏置来确定神经元是否应该被激活<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 它们将输入信号转换为输出的可微运算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 大多数激活函数都是非线性的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 由于激活函数是深度学习的基础<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>下面简要介绍一些常见的激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<ul>
<li>ReLU</li>
</ul>
<p><span class="markdown-them-math-inline">$ReLU(x) = max(x,0)$</span></p>
<p><img src="https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg" alt="../_images/output_mlp_76f463_21_0.svg"></p>
<ul>
<li>sigmoid</li>
</ul>
<p><span class="markdown-them-math-inline">$sigmoid(x)=\frac{1}{1+exp(-x)}$</span></p>
<p><img src="https://zh.d2l.ai/_images/output_mlp_76f463_51_0.svg" alt="../_images/output_mlp_76f463_51_0.svg"></p>
<ul>
<li>tanh</li>
</ul>
<p><span class="markdown-them-math-inline">$tanh(x) = \frac{1-exp(-2x)}{1+exp(-2x)}$</span></p>
<p><img src="https://zh.d2l.ai/_images/output_mlp_76f463_81_0.svg" alt="../_images/output_mlp_76f463_81_0.svg"></p>
<h2 id="32-误差">3.2 误差</h2>
<p><em>训练误差</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>training error<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>是指<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 模型在训练数据集上计算得到的误差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> <em>泛化误差</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>generalization error<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>是指<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 模型应用在同样从原始样本的分布中抽取的无限多数据样本时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>模型误差的期望<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>问题是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们永远不能准确地计算出泛化误差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这是因为无限多的数据样本是一个虚构的对象<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 在实际中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们只能通过将模型应用于一个独立的测试集来估计泛化误差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 该测试集由随机选取的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>未曾在训练集中出现的数据样本构成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="https://zh.d2l.ai/_images/capacity-vs-error.svg" alt="../_images/capacity-vs-error.svg"></p>
<h2 id="33-权重衰退">3.3 权重衰退</h2>
<p>在训练参数化机器学习模型时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> <em>权重衰减</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>weight decay<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>是最广泛使用的正则化的技术之一<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 它通常也被称为<span class="markdown-them-math-inline">$L_2$</span>正则化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>一种简单的方法是通过线性函数<span class="markdown-them-math-inline">$f(x)=w^Tx$</span> 中的权重向量的某个范数来度量其复杂性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 例如<span class="markdown-them-math-inline">$||w||^2$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 要保证权重向量比较小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 最常用方法是将其范数作为惩罚项加到最小化损失的问题中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 将原来的训练目标<em>最小化训练标签上的预测损失</em><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 调整为<em>最小化预测损失和惩罚项之和</em><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 现在<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果我们的权重向量增长的太大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 我们的学习算法可能会更集中于最小化权重范数<span class="markdown-them-math-inline">$||w||^2$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这正是我们想要的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>我们通过<em>正则化常数</em><span class="markdown-them-math-inline">$\lambda$</span>来描述这种权衡<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 这是一个非负超参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们使用验证数据拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><span class="markdown-them-math-inline">$L(w,b)+\frac{\lambda}{2}||w||_2$</span></p>
<p>此外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>为什么我们首先使用<span class="markdown-them-math-inline">$L_2$</span>范数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而不是<span class="markdown-them-math-inline">$L_1$</span>范数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 事实上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个选择在整个统计领域中都是有效的和受欢迎的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> <span class="markdown-them-math-inline">$L_2$</span>正则化线性模型构成经典的<em>岭回归</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>ridge regression<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> <span class="markdown-them-math-inline">$L_1$</span>正则化线性回归是统计学中类似的基本模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 通常被称为<em>套索回归</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>lasso regression<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 使用<span class="markdown-them-math-inline">$L_2$</span>范数的一个原因是它对权重向量的大分量施加了巨大的惩罚<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 在实践中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这可能使它们对单个变量中的观测误差更为稳定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 相比之下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$L_1$</span>惩罚会导致模型将权重集中在一小部分特征上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 而将其他权重清除为零<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这称为<em>特征选择</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>feature selection<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这可能是其他场景下需要的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><span class="markdown-them-math-inline">$L_2$</span>正则化回归的小批量随机梯度下降更新如下式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><span class="markdown-them-math-inline">$w = (1-\alpha\lambda) w - \frac{\alpha}{n}\sum_{i=1}^{n}(w^Tx^{(i)}+b-y^{(i)})$</span></p>
<p>我们仅考虑惩罚项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>优化算法在训练的每一步<em>衰减</em>权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 与特征选择相比<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>权重衰减为我们提供了一种连续的机制来调整函数的复杂度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 较小的<span class="markdown-them-math-inline">$\lambda$</span>值对应较少约束的<span class="markdown-them-math-inline">$w$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 而较大的<span class="markdown-them-math-inline">$\lambda$</span>值对<span class="markdown-them-math-inline">$w$</span>的约束更大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>在<code>pytorch</code>中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们在实例化优化器时直接通过<code>weight_decay</code>指定weight decay超参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 默认情况下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>PyTorch同时衰减权重和偏移<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这里我们只为权重设置了<code>weight_decay</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以偏置参数<span class="markdown-them-math-inline">$b$</span>不会衰减<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span>
        <span class="token punctuation">&#123;</span><span class="token string">"params"</span><span class="token punctuation">:</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">,</span><span class="token string">'weight_decay'</span><span class="token punctuation">:</span> wd<span class="token punctuation">&#125;</span><span class="token punctuation">,</span>
        <span class="token punctuation">&#123;</span><span class="token string">"params"</span><span class="token punctuation">:</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
<span class="token comment"># wd为lambda</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>正则化是处理过拟合的常用方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>在训练集的损失函数中加入惩罚项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>以降低学习到的模型的复杂度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
<li>保持模型简单的一个特别的选择是使用<span class="markdown-them-math-inline">$L_2$</span>惩罚的权重衰减<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这会导致学习算法更新步骤中的权重衰减<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
<li>权重衰减功能在深度学习框架的优化器中提供<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
<li>在同一训练代码实现中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不同的参数集可以有不同的更新行为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
</ul>
<h2 id="34-丢弃法">3.4 丢弃法</h2>
<p>当面对更多的特征而样本不足时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>线性模型往往会过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 相反<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当给出更多样本而不是特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常线性模型不会过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 不幸的是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>线性模型泛化的可靠性是有代价的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 简单地说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>线性模型没有考虑到特征之间的交互作用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 对于每个特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>线性模型必须指定正的或负的权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而忽略其他特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>那么关键的挑战就是如何注入这种噪声<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 一种想法是以一种<em>无偏向</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>unbiased<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的方式注入噪声<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这样在固定住其他层时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一层的期望值等于没有噪音时的值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>在每次训练迭代中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>他将从均值为零的分布<span class="markdown-them-math-inline">$\epsilon~N(0,\delta^2)$</span>采样噪声添加到输入<span class="markdown-them-math-inline">$x$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 从而产生扰动点<span class="markdown-them-math-inline">$x'=x+\epsilon$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 预期是<span class="markdown-them-math-inline">$E(x')=x$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>在标准暂退法正则化中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通过按保留<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>未丢弃<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的节点的分数进行规范化来消除每一层的偏差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 换言之<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每个中间活性值ℎ以<em>暂退概率</em><span class="markdown-them-math-inline">$p$</span>由随机变量ℎ′替换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如下所示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><span class="markdown-them-math-inline">$h'= 0 \ when\ p=0$</span></p>
<p><span class="markdown-them-math-inline">$h'=\frac{h}{1-p} \ otherwise$</span></p>
<p><img src="https://zh.d2l.ai/_images/dropout2.svg" alt="../_images/dropout2.svg"></p>
<p>对于深度学习框架的高级API<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们只需在每个全连接层之后添加一个<code>Dropout</code>层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 将暂退概率作为唯一的参数传递给它的构造函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 在训练时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>Dropout</code>层将根据指定的暂退概率随机丢弃上一层的输出<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>相当于下一层的输入<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 在测试时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>Dropout</code>层仅传递数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token comment"># 在第一个全连接层之后添加一个dropout层</span>
        nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout1<span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token comment"># 在第二个全连接层之后添加一个dropout层</span>
        nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout2<span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>暂退法在前向传播过程中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>计算每一内部层的同时丢弃一些神经元<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
<li>暂退法可以避免过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它通常与控制权重向量的维数和大小结合使用的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
<li>暂退法将活性值ℎ替换为具有期望值ℎ的随机变量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
<li>暂退法仅在训练期间使用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
</ul>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/paper-LCA%20with%20ML/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          结合生命周期评估和机器学习预测小农户小麦生产的环境影响
        
      </div>
    </a>
  
  
    <a href="/machine-learning-notes/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          机器学习课程笔记
        
      </div>
    </a>
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <p>Powered by Hexo & Theme Mashiro & GitHub Pages</p>
<p>Copyright © 2022-2024 HawYior. LICENSE CC BY-NC-SA 4.0.</p>
      
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/log" class="mobile-nav-link">Log</a>
  
</nav>
    

<script src="/js/clipboard.min.js"></script>
<script src="/js/jquery-1.4.3.min.js"></script>


<script src="/js/script.js"></script>






<script>
  MathJax = {
    options: {
      enableMenu: false
    },
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
    }
  };
</script>
<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    CommonHTML: {
      linebreaks: false
    }
  });
  </script> -->
<script type="text/javascript" id="MathJax-script" async
  src="/mathjax/tex-chtml.js">
</script>
<!-- <script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML">
</script> -->

  </div>
</body>
</html>