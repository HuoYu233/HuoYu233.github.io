<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>机器学习课程笔记 | HawYiorのBlog</title>
  <link rel="stylesheet" href="/css/fonts/Chinese-normal-normal.min.css">
  <link rel="stylesheet" href="/css/fonts/ChineseMono-normal-normal.min.css">
  <link rel="stylesheet" href="/css/fonts/Chinese-italic-normal.min.css">
  <link rel="stylesheet" href="/css/fonts/Chinese-normal-bold.min.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="description" content="吴恩达机器学习笔记">
  
  
  
    <link rel="shortcut icon" href="/logo.png">
  
  <link rel="stylesheet" href="/css/style.css">
  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div id="nav-outer">
  <nav id="main-nav" class="outer">
    <a id="main-nav-toggle" class="nav-icon"></a>
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
      <a class="main-nav-link" href="/log">Log</a>
    
    <div class="main-nav-space-between"></div>
    
  </nav>
</div>
<div id="header-title">
  <h1 id="logo-wrap">
    <a href="/" id="logo">HawYiorのBlog</a>
  </h1>
  
    <h2 id="subtitle-wrap">
      <a href="/" id="subtitle">we are here to code the world!</a>
    </h2>
  
</div>

      <div id="content" class="outer">
        <section id="main"><article id="post-machine-learning-notes" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/machine-learning-notes/" class="article-date">
  <time class="dt-published" datetime="2023-11-25T15:45:21.000Z" itemprop="datePublished">2023-11-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      机器学习课程笔记
    </h1>
  

      </header>
    
    
<div id="article-toc">
    <h2 class="widget-title">目录</h2>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#course-1"><span class="toc-number">1.</span> <span class="toc-text">Course 1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.1.1.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.1.2.</span> <span class="toc-text">多元线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">1.1.3.</span> <span class="toc-text">特征缩放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">1.1.4.</span> <span class="toc-text">特征工程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.</span> <span class="toc-text">分类-逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.1.</span> <span class="toc-text">sigmoid函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="toc-number">1.2.2.</span> <span class="toc-text">决策边界</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.3.</span> <span class="toc-text">成本函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-1"><span class="toc-number">1.2.4.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-number">1.2.5.</span> <span class="toc-text">过拟合问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">1.2.6.</span> <span class="toc-text">正则化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#course-2"><span class="toc-number">2.</span> <span class="toc-text">Course 2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.1.</span> <span class="toc-text">神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="toc-number">2.2.</span> <span class="toc-text">网络层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">2.3.</span> <span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%AD%A5%E9%AA%A4"><span class="toc-number">2.4.</span> <span class="toc-text">模型训练步骤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">2.5.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">2.6.</span> <span class="toc-text">多分类问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">2.7.</span> <span class="toc-text">高级优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#adam"><span class="toc-number">2.7.1.</span> <span class="toc-text">Adam</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%9A%84%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="toc-number">2.8.</span> <span class="toc-text">其他的网络层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">2.8.1.</span> <span class="toc-text">卷积层</span></a></li></ol></li></ol></li></ol>
</div>

    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="course-1">Course 1</h1>
<p>监督学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>输入特征x<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输出目标y<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>对数据集进行预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>分为<strong>回归</strong>和<strong>分类</strong></p>
<p>无监督学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>输入特征x<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>没有目标y<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对数据集进行聚类预测</p>
<h2 id="线性回归">线性回归</h2>
<p><span class="markdown-them-math-inline">$y^i = wx^i+b$</span></p>
<p>定义损失函数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>成本函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>需要最小化损失函数</p>
<p><span class="markdown-them-math-inline">$J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} {(y^i-\hat{y})}^2$</span></p>
<p>其中<span class="markdown-them-math-inline">$y^i$</span>为真实输出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\hat{y}$</span>为预测输出</p>
<h3 id="梯度下降">梯度下降</h3>
<p>需要最小会损失函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>需要使用梯度下降算法</p>
<p>定义学习率<code>learning_rate</code>为<span class="markdown-them-math-inline">$\alpha$</span>,一般<span class="markdown-them-math-inline">$\alpha \subseteq  [0,1]$</span></p>
<p><span class="markdown-them-math-inline">$w = w-  \alpha \frac{\partial{J(w,b)}}{\partial{w}}$</span></p>
<p><span class="markdown-them-math-inline">$b = b-  \alpha \frac{\partial{J(w,b)}}{\partial{b}}$</span></p>
<p><img src="/machine-learning-notes/pic-1.png" alt="pic-1"></p>
<p>如果<span class="markdown-them-math-inline">$\alpha$</span>太小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以得到答案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是时间过长</p>
<p>如果<span class="markdown-them-math-inline">$\alpha$</span>太大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>大交叉无法收敛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>甚至发散</p>
<p>当参数值每次更新时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$J(w,b)$</span>变小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>导数项<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>斜率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>也会变小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对于固定学习率<span class="markdown-them-math-inline">$\alpha$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>步长也会变小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>从而达到局部最优解</p>
<p>对导数项分别求导</p>
<p><span class="markdown-them-math-inline">$\frac{\partial{J(w,b)}}{\partial{w}} = \frac{1}{m} \sum_{i=1}^{m} (f(x^i)-y^i)x^i$</span></p>
<p><span class="markdown-them-math-inline">$\frac{\partial{J(w,b)}}{\partial{b}} = \frac{1}{m} \sum_{i=1}^{m} (f(x^i)-y^i)$</span></p>
<p>其中<span class="markdown-them-math-inline">$f(x^i) = wx^i+b$</span></p>
<p>对于线性回归损失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>他的损失函数图像是一个凸函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只有一个全局最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>没有局部最小值</p>
<p>选择合适得到学习率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就可以得到<span class="markdown-them-math-inline">$min(J(w,b))$</span></p>
<h3 id="多元线性回归">多元线性回归</h3>
<p>假设特征有<span class="markdown-them-math-inline">$n$</span>个<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>定义<span class="markdown-them-math-inline">$\vec{x} = \begin{bmatrix}
  x_1 &amp; x_2 &amp; x_3 &amp; ...
\end{bmatrix}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>参数<span class="markdown-them-math-inline">$\vec{w} = \begin{bmatrix}
  w_1 &amp; w_2 &amp; w_3 &amp; ...
\end{bmatrix}$</span></p>
<p>则<span class="markdown-them-math-inline">$f_{\vec{w},b}=\vec{w} \cdot \vec{x} +b$</span></p>
<p><code>·</code>为两个向量的点积(dot)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><span class="markdown-them-math-inline">$\vec{w} \cdot \vec{x} = w_1*x_1+w_2*x_2+....+w_n*x_n$</span></p>
<p><strong>矢量化</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>代码简洁<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>运行速度快</p>
<p>PS: 正规方程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>某些机器学习库在后端求<span class="markdown-them-math-inline">$w,b$</span>的方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只适用于线性回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且速度慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不要求掌握</p>
<h3 id="特征缩放">特征缩放</h3>
<p>加快梯度下降速度</p>
<p>避免特征的取值范围差异过大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>将其进行缩放</p>
<ul>
<li>
<p>除以最大值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$x_{1,scale} = \frac{x_1}{max}$</span></p>
</li>
<li>
<p>均值归一化</p>
<ul>
<li>求均值<span class="markdown-them-math-inline">$\mu$</span></li>
<li><span class="markdown-them-math-inline">$x_1 = \frac{x_1-\mu}{max-min}$</span></li>
</ul>
</li>
<li>
<p><code>Z-score</code>归一化</p>
<ul>
<li>求标准差<span class="markdown-them-math-inline">$\sigma$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>均值<span class="markdown-them-math-inline">$\mu$</span></li>
<li><span class="markdown-them-math-inline">$x_1 = \frac{x_1-\mu}{\sigma}$</span></li>
</ul>
</li>
</ul>
<p><strong>选择合适学习率</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>从0.001开始<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每次乘以3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对比<span class="markdown-them-math-inline">$J(w,b)$</span>与迭代次数的关系<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>选择合适的<span class="markdown-them-math-inline">$\alpha$</span></p>
<h3 id="特征工程">特征工程</h3>
<p>利用直觉设计新特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常通过转化与组合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使模型做出更准确的预测</p>
<p><strong>多项式回归</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>可以添加<span class="markdown-them-math-inline">$x^q$</span>项更好地拟合数据图像<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$f(x)=w_1x^3+w_2x^2+w_1x^1+b$</span></p>
<h2 id="分类-逻辑回归">分类-逻辑回归</h2>
<p>解决二分类问题</p>
<h3 id="sigmoid函数">sigmoid函数</h3>
<p>输出介于<span class="markdown-them-math-inline">$(0,1)$</span></p>
<p><span class="markdown-them-math-inline">$g(z)= \frac{1}{1+e^{-z}},z \subseteq R$</span></p>
<p><span class="markdown-them-math-inline">$f_{\vec{w},b}(\vec{x})=g(\vec{w} · \vec{x}+b) = \frac{1}{1+e^{-(\vec{w} · \vec{x}+b)}}$</span></p>
<h3 id="决策边界">决策边界</h3>
<p>以0.5作为阈值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当<span class="markdown-them-math-inline">$\vec{w} · \vec{x}+b \ge 0$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>取值1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>当<span class="markdown-them-math-inline">$\vec{w} · \vec{x}+b &lt;0$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>取值0</p>
<p><span class="markdown-them-math-inline">$\vec{w} · \vec{x}+b = 0$</span>称为决策边界</p>
<p>也适用于多项式回归</p>
<h3 id="成本函数">成本函数</h3>
<p>如果使用平方误差成本函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>有多个局部最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$J(w,b)$</span>是不是凸函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不适用于逻辑回归</p>
<p>定义<span class="markdown-them-math-inline">$J(w,b)=\frac{1}{m}\sum_{i-1}^{m}L(f_{w,b}(x^{(i)},y^{(i)})$</span></p>
<p>其中</p>
<p><span class="markdown-them-math-inline">$L(f_{w,b}(x^{(i)},y^{(i)})=-log(f_{w,b}(x^{(i)})) \quad if \quad y^{(i)}=1$</span></p>
<p><span class="markdown-them-math-inline">$L(f_{w,b}(x^{(i)},y^{(i)})=-log(1-f_{w,b}(x^{(i)})) \quad if \quad y^{(i)}=0$</span></p>
<p><strong>简化</strong>成本函数</p>
<p><span class="markdown-them-math-inline">$L(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))$</span></p>
<p>得到</p>
<p><span class="markdown-them-math-inline">$J(w,b) = -\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))$</span></p>
<p>使得成本函数是凸函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>便于实现梯度下降</p>
<h3 id="梯度下降-1">梯度下降</h3>
<p>对导数项分别求导</p>
<p><span class="markdown-them-math-inline">$\frac{\partial{J(w,b)}}{\partial{w}} = \frac{1}{m} \sum_{i=1}^{m} (f(x^i)-y^i)x^i$</span></p>
<p><span class="markdown-them-math-inline">$\frac{\partial{J(w,b)}}{\partial{b}} = \frac{1}{m} \sum_{i=1}^{m} (f(x^i)-y^i)$</span></p>
<p>其中<span class="markdown-them-math-inline">$f(x^i) =  \frac{1}{1+e^{-(\vec{w} · \vec{x}+b)}}$</span></p>
<p>可以使用相似方法进行特征缩放</p>
<h3 id="过拟合问题">过拟合问题</h3>
<p>过拟合虽然可能完美通过训练集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是有高方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>应该避免欠拟合<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>高偏差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>和过拟合<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>高方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><strong>解决过拟合</strong></p>
<ul>
<li>收集更多训练数据</li>
<li>选择特征的一个子集</li>
<li>正则化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>减小参数<span class="markdown-them-math-inline">$w_j$</span>的值</li>
</ul>
<h3 id="正则化">正则化</h3>
<p>如果不知道哪个特征是重要的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>惩罚所有特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>防止过拟合</p>
<p><span class="markdown-them-math-inline">$J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} {(y^i-\hat{y})}^2 + \frac{\lambda}{\alpha m}\sum_{j=1}^{n} {w_j}^2$</span></p>
<p>其中<span class="markdown-them-math-inline">$\lambda$</span>为正则化参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\alpha$</span>为学习率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>缩放得</p>
<p><span class="markdown-them-math-inline">$J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} {(y^i-\hat{y})}^2 + \frac{\lambda}{2 m}\sum_{j=1}^{n} {w_j}^2$</span></p>
<p>参数<span class="markdown-them-math-inline">$b$</span>是否正则化无关紧要</p>
<p><strong>需要选择合适的<span class="markdown-them-math-inline">$\lambda$</span></strong></p>
<ul>
<li>正则化线性回归</li>
</ul>
<p>对<span class="markdown-them-math-inline">$J(w,b)$</span>求偏导不断同步更新w,b的值</p>
<p><span class="markdown-them-math-inline">$\frac{\partial{J(w,b)}}{\partial{w}} = \frac{1}{m} \sum_{i=1}^{m} (f(x^i)-y^i)x^i+\frac{\lambda}{m}\sum_{j=1}^{m}{w_j}$</span></p>
<p><span class="markdown-them-math-inline">$w = w-  \alpha (\frac{1}{m} \sum_{i=1}^{m} (f(x^i)-y^i)x^i+\frac{\lambda}{m}\sum_{j=1}^{m}{w_j}) = (1-\alpha \frac{\lambda}{m})w+.....$</span></p>
<ul>
<li>正则化逻辑回归</li>
</ul>
<p><span class="markdown-them-math-inline">$J(w,b) = -\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))+  \frac{\lambda}{2 m}\sum_{j=1}^{n} {w_j}^2$</span></p>
<p>求导式和线性回归相同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只是需要注意</p>
<p><span class="markdown-them-math-inline">$f(x^i) =  \frac{1}{1+e^{-(\vec{w} · \vec{x}+b)}}$</span></p>
<h1 id="course-2">Course 2</h1>
<h2 id="神经网络">神经网络</h2>
<p>起源于设计算法来模拟人脑活动<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>21世纪定义为<strong>深度学习</strong></p>
<p>输入层-&gt;隐藏层-&gt;输出层</p>
<h2 id="网络层">网络层</h2>
<p>每一层输入向量<span class="markdown-them-math-inline">$\vec{x}$</span>或<span class="markdown-them-math-inline">$\vec{a}_{i-1}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>经过当前层中多个逻辑回归处理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输出新的向量<span class="markdown-them-math-inline">$\vec{a}^{[l]}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>进入到下一层/输出结果</p>
<p>即<span class="markdown-them-math-inline">$a_j^{[l]} = g(\vec{w}_j^{[l]} \cdot \vec{a}^{[l-1]} + b_j^{[l]})$</span></p>
<p>j表示单元序号<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>l表示层数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$g(x)$</span>为<code>sigmod</code>函数</p>
<h2 id="前向传播">前向传播</h2>
<p>从输入初步传递到输出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即为前向传播</p>
<p><strong>一般实现</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">dense</span><span class="token punctuation">(</span>a_in<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">,</span> g<span class="token punctuation">)</span><span class="token punctuation">:</span>
	units <span class="token operator">=</span> W<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
	a_out <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>units<span class="token punctuation">)</span>
	<span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>units<span class="token punctuation">)</span><span class="token punctuation">:</span>
		w <span class="token operator">=</span> W<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span>
		z <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>w<span class="token punctuation">,</span> a_in<span class="token punctuation">)</span> <span class="token operator">+</span> b
		a_out<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> g<span class="token punctuation">(</span>z<span class="token punctuation">)</span>
	<span class="token keyword">return</span> a_out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">sequential</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    a1 <span class="token operator">=</span> dense<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">)</span>
    a2 <span class="token operator">=</span> dense<span class="token punctuation">(</span>a1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">)</span>
    a3 <span class="token operator">=</span> dense<span class="token punctuation">(</span>a2<span class="token punctuation">,</span> W3<span class="token punctuation">,</span> b3<span class="token punctuation">)</span>
    f_x <span class="token operator">=</span> a3
    <span class="token keyword">return</span> f_x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>使用框架进行矢量化加速</strong></p>
<h2 id="模型训练步骤">模型训练步骤</h2>
<ol>
<li>指定如何在给定输入X和参数的情况下计算输出</li>
<li>指定损失函数</li>
<li>训练模型以最小化损失函数</li>
</ol>
<p>二元交叉熵损失</p>
<p><span class="markdown-them-math-inline">$L(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))$</span></p>
<p>通过反向传播计算偏导数</p>
<h2 id="激活函数">激活函数</h2>
<p>用<code>ReLU</code>函数代替<code>sigmoid</code>激活<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$g(z) = max(0,z)$</span></p>
<p><img src="/machine-learning-notes/pic-2.png" alt="pic-2"></p>
<p><strong>如何选择合适的激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span></strong></p>
<p>取决于要预测的y</p>
<ul>
<li>二分类/回归中y可正可负 ——&gt; sigmoid</li>
<li>回归中y大于等于0 ——&gt; ReLU</li>
<li>对于隐藏层建议使用ReLU</li>
</ul>
<p><code>ReLU</code>常用且更快</p>
<p><strong>为什么需要激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span></strong></p>
<p>如果不使用<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即使用线性的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>得到的结果显然可以仅仅使用线性回归或者逻辑回归求解</p>
<h2 id="多分类问题">多分类问题</h2>
<p><strong>softmax回归算法</strong></p>
<p><span class="markdown-them-math-inline">$z_i=\vec{w_1}·\vec{x_i}+b_i$</span></p>
<p><span class="markdown-them-math-inline">$a_1=\frac{e^{z_1}}{e^{z_1}+...+e^{z_n}} = P(y=1|\vec{x})$</span></p>
<p>即<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>设有N个分类</p>
<p><span class="markdown-them-math-inline">$z_i=\vec{w_1}·\vec{x_i}+b_i$</span></p>
<p><span class="markdown-them-math-inline">$a_i = \frac{e^{z_i}}{\sum_{k=1}^{N} e^{z_i}}=P(y=i|\vec{x})$</span></p>
<p><span class="markdown-them-math-inline">$a_1+a_2+...+a_N=1$</span></p>
<p><strong>softmax损失函数</strong></p>
<p><span class="markdown-them-math-inline">$loss(a_1,a_2,...,a_N,y) = \left\{\begin{matrix}
-log(a_1) \quad if \quad y=1\\
-log(a_2) \quad if \quad y=2 \\
... \\
-log(a_N) \quad if \quad y=N
\end{matrix}\right.$</span></p>
<p><strong>softmax改进</strong></p>
<p>不先计算出<span class="markdown-them-math-inline">$a_i$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>再带入损失函数</p>
<p>而是直接<span class="markdown-them-math-inline">$loss=-log(\frac{e^{z_1}}{e_{z_1}+...+e_{z_N}})$</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>SparseCategoricalCrossEntropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>多标签分类</strong></p>
<p>将每个标签分为一个二分类问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>容易混淆</p>
<h2 id="高级优化方法">高级优化方法</h2>
<h3 id="adam">Adam</h3>
<p>可以自动调整学习率<span class="markdown-them-math-inline">$\alpha$</span></p>
<p>对于每个参数都有一个<span class="markdown-them-math-inline">$\alpha$</span></p>
<h2 id="其他的网络层">其他的网络层</h2>
<h3 id="卷积层">卷积层</h3>
<ul>
<li>加快计算速度</li>
<li>需要更少的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不容易过拟合</li>
</ul>
<p>有多个卷积层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即卷积神经网络</p>
<p>每一层的单元只查看输入的一部分</p>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/deep-learning-notes/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          神经网络深度学习笔记
        
      </div>
    </a>
  
  
    <a href="/redis/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          Redis学习笔记
        
      </div>
    </a>
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <p>Powered by Hexo & Theme Mashiro & GitHub Pages</p>
<p>Copyright © 2022-2024 HawYior. LICENSE CC BY-NC-SA 4.0.</p>
      
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/log" class="mobile-nav-link">Log</a>
  
</nav>
    

<script src="/js/clipboard.min.js"></script>
<script src="/js/jquery-1.4.3.min.js"></script>


<script src="/js/script.js"></script>






<script>
  MathJax = {
    options: {
      enableMenu: false
    },
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
    }
  };
</script>
<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    CommonHTML: {
      linebreaks: false
    }
  });
  </script> -->
<script type="text/javascript" id="MathJax-script" async
  src="/mathjax/tex-chtml.js">
</script>
<!-- <script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML">
</script> -->

  </div>
</body>
</html>